import subprocess

# Install the patool package
subprocess.check_call(["pip", "install", "patool"])

# Now you can import patool and use it in your Streamlit app
import patool

# Your Streamlit code continues from here...

# -*- coding: utf-8 -*-
"""book recommendation system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t-rr40C87ti0Vtxo8oLU1WBdUb5jDoT6
"""

popularity.merge(books,on='Book-Title').drop_duplicates('Book-Title')

!pip install patool

import patoolib

patoolib.extract_archive('/content/Ratings.csv.zip')

patoolib.extract_archive('/content/Users.csv.zip')

patoolib.extract_archive('/content/Books.csv.zip')

import numpy as np
import pandas as pd

import seaborn as sns

books=pd.read_csv('Books.csv')
users=pd.read_csv('Users.csv')
ratings=pd.read_csv('Ratings.csv')

books.head()

users.head()

ratings.head()

print(books.shape)
print(users.shape)
print(ratings.shape)

books.isnull().sum()

users.isnull().sum()

ratings.isnull().sum()

books.duplicated().sum()

ratings.duplicated().sum()

users.duplicated().sum()

"""***EDA-EXPLORATORY DATA ANALYSIS***"""

sns.heatmap(users.isnull(),yticklabels=False,cbar=True,cmap='viridis')

"""*By using heatmaps we can represent different typesof values or intensities of datapoints by making them easy to pointout and to spot patterns,trends,outliers in the data. It helps to understand the complex and difficult to understand in intuitive way.*

Here we can observe that the age contain large number of*** missing data*** which may cannot be replaced using imputation so we m ay drop off the data label

> If data is missing for a large proportion of the observations, it may be best to discard the variable entirely if it is insignificant.https://www.mastersindatascience.org/learning/how-to-deal-with-missing-data/#:~:text=When%20dealing%20with%20missing%20data,of%20missing%20data%20is%20low.
"""

sns.heatmap(books.isnull(),yticklabels=False,cbar=True,cmap='viridis')

sns.heatmap(ratings.isnull(),yticklabels=False,cbar=True,cmap='viridis')

sns.set_style('whitegrid')
sns.countplot(x='Book-Rating',data=ratings)

zero_ratings = ratings[ratings['Book-Rating']==0 ]
print(zero_ratings)



sns.displot(users['Age'].dropna(),kde=False,color='darkred',bins=30,)

"""Here we can observe that people between age of 30-50 are purchasing books in large number

***DATA CLEANING***
"""

mean_age = users['Age'].mean()
users['Age'] = users['Age'].fillna(mean_age)
print(users['Age'])

sns.heatmap(users.isnull(),yticklabels=False,cbar=True,cmap='viridis')

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.boxplot(users['Age'])
plt.title('Boxplot of Age')
plt.ylabel('Age')
plt.show()

users.drop('Age',axis=1,inplace=True)

users.head()

"""***POPULARITY BASED RECOMMENDER SYSTEM***"""

merged=ratings.merge(books,on='ISBN')
merged.head()

ratings_number=merged.groupby('Book-Title').count()['Book-Rating'].reset_index()
ratings_number.rename(columns={'Book-Rating':'number_of_ratings'},inplace=True)
ratings_number

average_rating=merged.groupby('Book-Title').mean()['Book-Rating'].reset_index()
average_rating.rename(columns={'Book-Rating':'number_of_ratings'},inplace=True)
average_rating

""" explaining error:
 The error message indicates that there's a problem in  converting the values in the 'Book-Rating' column to numeric values. Specifically, it seems that the values in the 'Book-Rating' column are not numeric, but rather strings or other non-numeric data types.
"""

import pandas as pd

# Assuming you've already merged the datasets and created a DataFrame named 'merged'

# Step 1: Check the data types in the 'Book-Rating' column
# Convert to numeric, coerce errors to NaN
merged['Book-Rating'] = pd.to_numeric(merged['Book-Rating'], errors='coerce')

# Step 2: Remove rows with NaN values in 'Book-Rating' column
merged.dropna(subset=['Book-Rating'], inplace=True)

# Step 3: Calculate the average rating
average_rating = merged.groupby('Book-Title')['Book-Rating'].mean().reset_index()

# Step 4: Rename the column
average_rating.rename(columns={'Book-Rating': 'average_rating'}, inplace=True)

# Display the resulting DataFrame
print(average_rating)

popularity=ratings_number.merge(average_rating,on='Book-Title')
popularity

popularity=popularity[popularity['number_of_ratings']>=250].sort_values('average_rating',ascending=False).head(50)

"""for same book different ISBN numbers"""

popularity=popularity.merge(books,on='Book-Title').drop_duplicates('Book-Title')[['Book-Title','Book-Author','number_of_ratings','average_rating','Image-URL-M']]

popularity

"""***Collaborative Filtering Based Recommended system***"""

x=merged.groupby('User-ID').count()['Book-Rating']>200
educated_users=x[x].index

FR=merged[merged['User-ID'].isin(educated_users)]

"""FR=Filtered rating"""

y=FR.groupby('Book-Title').count()['Book-Rating']>=50
famous_books=y[y].index

final_ratings=FR[FR['Book-Title'].isin(famous_books)]

final_ratings

pivot_table=final_ratings.pivot_table(index='Book-Title',columns='User-ID',values='Book-Rating')

pivot_table.fillna(0,inplace=True)

pivot_table

from sklearn.metrics.pairwise import cosine_similarity

similarity=cosine_similarity(pivot_table)

similarity.shape

def recommend(book_name):
  # index fetching
  index=np.where(pivot_table.index==book_name)[0][0]
  similar_items=sorted(list(enumerate(similarity[index])),key=lambda x:x[1],reverse=True)[1:6]

  for i in similar_items:
    print(pivot_table.index[i[0]])

recommend('1984')

pivot_table.index[545]

popularity['Image-URL-M'][0]

import pickle
pickle.dump(popularity,open('popular.pkl','wb'))
